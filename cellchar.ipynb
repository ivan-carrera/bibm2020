{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# CellChar: A SVM-based characterization method for cellular lines using text processing\n",
    "\n",
    "## Iván Carrera, Eduardo Tejera, and Inês Dutra\n",
    "### Departamento de Informática y Ciencias de la Computación, Escuela Politécnica Nacional. Quito, Ecuador.\n",
    "### Departamento de Ciencia de Computadores, Universidade do Porto, Portugal.\n",
    "### Grupo de Quimio-Bioinformática, Universidad de Las Américas. Quito, Ecuador.\n",
    "\n",
    "### Programa de Doutoramento em Ciência de Computadores FCUP.\n",
    "\n",
    "#### Characterizing Cellular lines\n",
    "The goal of this work is to characterize cellular lines from processing the related scientific literature\n",
    "\n",
    "##### Web mining\n",
    "There are two main source databases for cellular lines: [Cellosaurus](https://web.expasy.org/cellosaurus/)\n",
    "and [ChEMBL](https://www.ebi.ac.uk/chembl/).\n",
    "First of all, we have to identify the cellular lines. File contains a list of cellular lines, with their synonyms"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Processing Cellosaurus v34"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cellosaurus_file = 'data/cellosaurus/cellosaurus.xml'\n",
    "print('Cellosaurus file:', cellosaurus_file)\n",
    "with open(cellosaurus_file, 'r') as file_:\n",
    "    lines = file_.readlines()\n",
    "    for i in range(107,135):\n",
    "        print(lines[i].strip('\\n'))\n",
    "\n",
    "import xml.etree.ElementTree as ET\n",
    "tree = ET.parse(cellosaurus_file)\n",
    "root = tree.getroot()\n",
    "\n",
    "cell_dict = dict()\n",
    "for cell_line in root.find('cell-line-list').findall('cell-line'):\n",
    "    accession = cell_line.find('accession-list').find('accession')\n",
    "    name_list = list()\n",
    "    for name in cell_line.find('name-list').findall('name'):\n",
    "        name_list.append(name.text)\n",
    "    species_list = list()\n",
    "    for species in cell_line.find('species-list').findall('cv-term'):\n",
    "        species_list.append(species.attrib['accession'])\n",
    "    reference_list = list()\n",
    "    try:\n",
    "        for reference in cell_line.find('reference-list').findall('reference'):\n",
    "            if 'PubMed' in reference.attrib['resource-internal-ref']:\n",
    "                reference_list.append(reference.attrib['resource-internal-ref'].replace('PubMed=',''))\n",
    "    except AttributeError:\n",
    "        pass\n",
    "    cell_dict[accession.text] = {'accession': accession.text, 'cell_name': name_list, 'species': species_list,\n",
    "                      'reference': reference_list}\n",
    "\n",
    "print('\\nCell line examples:')\n",
    "for cell_ in list(cell_dict.keys())[:2]:\n",
    "    print(cell_, cell_dict[cell_])\n",
    "\n",
    "print('\\nCellosaurus contains information about', len(cell_dict), 'cellular lines')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The goal of this work is to characterize cellular lines from processing the related scientific literature.\n",
    "In this context, we can understand that verified references are those that appear in cellosaurus as reference.\n",
    "This will be our _Ground Truth_."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_cells_with_min_gtruth_refs(reference_min=0):\n",
    "    gtruth_idx = list()\n",
    "    for cell in cell_dict:\n",
    "        if len(cell_dict[cell]['reference']) >= reference_min:\n",
    "            gtruth_idx.append(cell)\n",
    "    return gtruth_idx\n",
    "\n",
    "gtruth_idx = list()\n",
    "for i in range(20):\n",
    "    gtruth_idx = get_cells_with_min_gtruth_refs(i)\n",
    "    print(len(gtruth_idx),'cell lines with', i, 'or more references')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, using Entrez API, we can check how many references in PubMed are related to the cellular lines.\n",
    "First, we transform the list of names for a cellular line into a query for the PubMed search API."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import parse\n",
    "for cell_ in gtruth_idx[:4]:\n",
    "    print(cell_, cell_dict[cell_]['cell_name'])\n",
    "    print(parse.list_toquery(cell_dict[cell_]['cell_name']))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can see that there are cell lines with numeric names. These numeric names should be removed from name lists\n",
    "because they can cause that search queries return false results."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def remove_numeric_names(namelist):\n",
    "    newlist = list()\n",
    "    for name in namelist:\n",
    "        if not str(parse.replace_spchars(name)).replace(' ','').isnumeric():\n",
    "            newlist.append(name)\n",
    "    return newlist\n",
    "\n",
    "list_to_pop = list()\n",
    "for cell_ in cell_dict:\n",
    "    newlist = remove_numeric_names(cell_dict[cell_]['cell_name'])\n",
    "    if len(newlist) == 0:\n",
    "        list_to_pop.append(cell_)\n",
    "    cell_dict[cell_]['cell_name'] = newlist\n",
    "\n",
    "print('There are',len(list_to_pop), 'cell lines with only numeric names.')\n",
    "\n",
    "for cell_ in list_to_pop:\n",
    "    try:\n",
    "        cell_dict.pop(cell_)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "gtruth_idx = get_cells_with_min_gtruth_refs(1)\n",
    "\n",
    "print('We are left with', len(cell_dict), 'cell lines.')\n",
    "print(len(gtruth_idx),'cell lines with', 1, 'or more references')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We have removed numeric names from cell lines.\n",
    "\n",
    "Now, using Entrez API, we can check how many references in PubMed are related to the cellular lines.\n",
    "First, we transform the list of names for a cellular line into a query for the PubMed search API."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for cell_ in gtruth_idx[:4]:\n",
    "    print(cell_, cell_dict[cell_]['cell_name'])\n",
    "    print(parse.list_toquery(cell_dict[cell_]['cell_name']))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We use that query to retrieve PMIDs by using Entrez API"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for cell_ in gtruth_idx[:10]:\n",
    "    print('_'*80)\n",
    "    list_ = cell_dict[cell_]['cell_name']\n",
    "    print(cell_, list_)\n",
    "    query = parse.list_toquery(list_)\n",
    "    print(query)\n",
    "    idlist = parse.search_idlist(query)\n",
    "    print(cell_, len(idlist), 'references in PubMed')\n",
    "    print(cell_, len(cell_dict[cell_]['reference']), 'references in Ground Truth')\n",
    "    comm_ref = list(set(cell_dict[cell_]['reference']).intersection(set(idlist)))\n",
    "    print(cell_, len(comm_ref), 'references in Ground Truth are in PubMed')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, we can store abstracts to disk.\n",
    "We will retrieve abstracts in two directories: one for Ground Truth, and other for PubMed references."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for cell_ in gtruth_idx[:10]:\n",
    "    parse.process_gt_pm(cell_dict[cell_])\n",
    "\n",
    "# This code should be uncommmented if you wish to retrieve all abstracts\n",
    "# import multiprocessing\n",
    "# pool = multiprocessing.Pool(processes=8)\n",
    "# pool.map(func=parse.process_gt_pm, iterable=cell_dict)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We now have a full database with retrieved abstracts.\n",
    "However, for compatibility and standard processing, we should parse json files into two CSV files."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "gtruth_dir = 'data/cell_json_gt/'\n",
    "gtruth_df = pd.DataFrame(columns=['title', 'index', 'document', 'cell_id'])\n",
    "i = 0\n",
    "for cell_ in gtruth_idx:\n",
    "    gtruth_file = gtruth_dir + cell_ + '.json'\n",
    "    if os.path.exists(gtruth_file):\n",
    "        cell_gtruth_ = json.load(fp=open(gtruth_file, 'r'))\n",
    "        docs = pd.DataFrame.from_dict(cell_gtruth_['documents'])\n",
    "        gtruth_df = pd.concat([gtruth_df, docs]).reset_index(drop=True)\n",
    "        i = i + 1\n",
    "\n",
    "print(i, 'files in', gtruth_dir)\n",
    "gtruth_df.to_csv(path_or_buf='data/gt_df.csv', sep=',', header=True, index=True)\n",
    "\n",
    "pm_dir = 'data/cell_json_pm/'\n",
    "pm_filelist = os.listdir(pm_dir)\n",
    "\n",
    "reference_df = pd.DataFrame(columns=['title', 'index', 'document', 'cell_id'])\n",
    "i = 0\n",
    "for reference_file in pm_filelist:\n",
    "    cell_reference_ = json.load(fp=open(pm_dir + reference_file, 'r'))\n",
    "    docs = pd.DataFrame.from_dict(cell_reference_['documents'])\n",
    "    reference_df = pd.concat([reference_df, docs]).reset_index(drop=True)\n",
    "    i = i + 1\n",
    "\n",
    "print(i, 'files in', pm_dir)\n",
    "reference_df.to_csv(path_or_buf='data/pm_df.csv', sep=',', header=True, index=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If you don't want to have to retrieve all that information, you can read directly from the CSV file."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gtruth_file = 'data/gt_df.csv'\n",
    "reference_file = 'data/pm_df.csv'\n",
    "\n",
    "gtruth_df = pd.read_csv(gtruth_file, sep=',', error_bad_lines=False, encoding=\"latin-1\",\n",
    "                         index_col=0, header=0)\n",
    "reference_df = pd.read_csv(reference_file, sep=',', error_bad_lines=False, encoding=\"latin-1\",\n",
    "                         index_col=0, header=0)\n",
    "\n",
    "print('GTruth Document table has information on', gtruth_df.shape[0], 'documents of',\n",
    "      gtruth_df['cell_id'].unique().shape[0], 'cell lines')\n",
    "print('Reference Document table has information on', reference_df.shape[0], 'documents of',\n",
    "      reference_df['cell_id'].unique().shape[0], 'cell lines')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now that we have loaded the corpus, we can process it.\n",
    "First, we remove all references and ground truth that is not common."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "gt_count = gtruth_df.groupby(by=['cell_id'])['cell_id'].count()\n",
    "gtruth_df = gtruth_df[gtruth_df.cell_id.isin(gt_count[gt_count > 2].index)]\n",
    "pm_count = reference_df.groupby(by=['cell_id'])['cell_id'].count()\n",
    "reference_df = reference_df[reference_df.cell_id.isin(pm_count[pm_count > 2].index)]\n",
    "\n",
    "gtruth_cells = gtruth_df['cell_id'].unique()\n",
    "reference_cells = reference_df['cell_id'].unique()\n",
    "cell_intersection = set(gtruth_cells).intersection(set(reference_cells))\n",
    "\n",
    "print('Datasets have', len(cell_intersection), 'common cell lines')\n",
    "\n",
    "gtruth_df = gtruth_df[gtruth_df.cell_id.isin(cell_intersection)].reset_index(drop=True)\n",
    "reference_df = reference_df[reference_df.cell_id.isin(cell_intersection)].reset_index(drop=True)\n",
    "\n",
    "print('GTruth Document table has information on', gtruth_df.shape[0], 'documents of',\n",
    "      gtruth_df['cell_id'].unique().shape[0], 'cell lines')\n",
    "print('Reference Document table has information on', reference_df.shape[0], 'documents of',\n",
    "      reference_df['cell_id'].unique().shape[0], 'cell lines')\n",
    "\n",
    "reference_df = pd.concat([reference_df, gtruth_df]).reset_index(drop=True)\n",
    "pm_count = reference_df.groupby(by=['cell_id'])['cell_id'].count()\n",
    "print('Every cell line has at least', min(pm_count), 'documents')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We define _crossval_ method for processing the dataframe.\n",
    "_crossval_ method filters cell lines with _min_references_ documents."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def crossval(df, min_references, stemmed=False):\n",
    "    pm_count = df.groupby(by=['cell_id'])['cell_id'].count()\n",
    "    df = df[df.cell_id.isin(pm_count[pm_count >= min_references].index)]\n",
    "    df = df[df.cell_id.isin(pm_count[pm_count < 500].index)]\n",
    "    print(df.shape[0], 'abstracts in corpus')\n",
    "    pm_count = df.groupby(by=['cell_id'])['cell_id'].count()\n",
    "    print('Every cell line has at least', min(pm_count), 'documents')\n",
    "\n",
    "    if stemmed:\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_corpus = list()\n",
    "        for doc in df['document']:\n",
    "            doc = doc.replace('.', '').replace(',', '').replace(';', '').replace(':', '')\n",
    "            stemmed_corpus.append(' '.join([stemmer.stem(x) for x in doc.split()]))\n",
    "        corpus = stemmed_corpus\n",
    "    else:\n",
    "        corpus = df['document']\n",
    "\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=1.0, min_df=2, strip_accents='ascii', stop_words='english',\n",
    "                                 token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
    "    dtm = vectorizer.fit_transform(corpus)\n",
    "    print('DTM shape:', dtm.shape)\n",
    "    wordfreq = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "    print('Top 20 words')\n",
    "    print(sorted(wordfreq.items(), key=lambda x: x[1])[:20])\n",
    "    reference_vocab = vectorizer.get_feature_names()\n",
    "    print(len(reference_vocab), 'words')\n",
    "\n",
    "    X, y = dtm, df['cell_id']\n",
    "    scoring = ['precision_micro', 'recall_micro']\n",
    "    param_grid = {'C': [1, 10], 'kernel': ('linear', 'rbf')}\n",
    "    svc = SVC(decision_function_shape='ovr')\n",
    "    clf = GridSearchCV(estimator=svc, param_grid=param_grid, return_train_score=True, cv=10, n_jobs=-1,\n",
    "                       scoring=scoring, refit='recall_micro', verbose=1)\n",
    "    clf = clf.fit(X=X, y=y, groups=df['cell_id'])\n",
    "    return {'clf': clf, 'estimator': clf.best_estimator_, 'pm_count': pm_count}\n",
    "\n",
    "\n",
    "results_min_ref = dict()\n",
    "for min_ref in [10, 50, 100]:\n",
    "    print('_'*80)\n",
    "    print(min_ref, 'minimum references')\n",
    "    results_min_ref[min_ref] = crossval(df=reference_df, min_references=min_ref, stemmed=False)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We plot micro-averaged precision for Non-Stemmed corpus.\n",
    "This results show that the best estimator is _C:1-kernel:linear_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "clf_10 = results_min_ref[10]['clf']\n",
    "clf_50 = results_min_ref[50]['clf']\n",
    "clf_100 = results_min_ref[100]['clf']\n",
    "\n",
    "min_refs = [10, 50, 100]\n",
    "labels = ['C:1-kernel:linear', 'C:1-kernel:rbf', 'C:10-kernel:linear', 'C:10-kernel:rbf']\n",
    "markers = ['^', '.', 'v', 's']\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.ylabel('Micro-Averaged Precision')\n",
    "plt.xlabel('Minimum number of documents')\n",
    "for i in range(4):\n",
    "    x = pd.array(min_refs)\n",
    "    y = clf_10.cv_results_['mean_test_precision_micro'][i], clf_50.cv_results_['mean_test_precision_micro'][i], \\\n",
    "        clf_100.cv_results_['mean_test_precision_micro'][i]\n",
    "    e = clf_10.cv_results_['std_test_precision_micro'][i], clf_50.cv_results_['std_test_precision_micro'][i], \\\n",
    "        clf_100.cv_results_['std_test_precision_micro'][i]\n",
    "    plt.errorbar(x, y, e, linestyle='None', marker=markers[i], label=labels[i], capsize=3)\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "plt.savefig('data/img/precision_.png')\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We repeat the process for stemmed corpus.\n",
    "This results show that the best estimator is _C:1-kernel:linear_."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "results_min_ref = dict()\n",
    "for min_ref in [10, 50, 100]:\n",
    "    print('_'*80)\n",
    "    print(min_ref, 'minimum references')\n",
    "    results_min_ref[min_ref] = crossval(df=reference_df, min_references=min_ref, stemmed=True)\n",
    "\n",
    "clf_10 = results_min_ref[10]['clf']\n",
    "clf_50 = results_min_ref[50]['clf']\n",
    "clf_100 = results_min_ref[100]['clf']\n",
    "\n",
    "min_refs = [10, 50, 100]\n",
    "labels = ['C:1-kernel:linear', 'C:1-kernel:rbf', 'C:10-kernel:linear', 'C:10-kernel:rbf']\n",
    "markers = ['^', '.', 'v', 's']\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.ylabel('Micro-Averaged Precision')\n",
    "plt.xlabel('Minimum number of documents')\n",
    "for i in range(4):\n",
    "    x = pd.array(min_refs)\n",
    "    y = clf_10.cv_results_['mean_test_precision_micro'][i], clf_50.cv_results_['mean_test_precision_micro'][i], \\\n",
    "        clf_100.cv_results_['mean_test_precision_micro'][i]\n",
    "    e = clf_10.cv_results_['std_test_precision_micro'][i], clf_50.cv_results_['std_test_precision_micro'][i], \\\n",
    "        clf_100.cv_results_['std_test_precision_micro'][i]\n",
    "    plt.errorbar(x, y, e, linestyle='None', marker=markers[i], label=labels[i], capsize=3)\n",
    "    plt.legend(loc='upper left')\n",
    "\n",
    "plt.savefig('data/img/precision_stemmed.png')\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define a pipeline for calculating Micro-Averaged precision varying the minimum number of documents per class.\n",
    "We plot micro-averaged precision for Non-Stemmed corpus."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_validate\n",
    "\n",
    "def min_ref_pipeline(df, min_references, stemmed=False):\n",
    "    pm_count = df.groupby(by=['cell_id'])['cell_id'].count()\n",
    "    df = df[df.cell_id.isin(pm_count[pm_count >= min_references].index)]\n",
    "    df = df[df.cell_id.isin(pm_count[pm_count < 500].index)]\n",
    "    print(df.shape[0], 'abstracts in corpus')\n",
    "    pm_count = df.groupby(by=['cell_id'])['cell_id'].count()\n",
    "    print(pm_count.shape[0], 'cell lines')\n",
    "    print('Every cell line has at least', min(pm_count), 'documents')\n",
    "\n",
    "    if stemmed:\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_corpus = list()\n",
    "        for doc in df['document']:\n",
    "            doc = doc.replace('.', '').replace(',', '').replace(';', '').replace(':', '')\n",
    "            stemmed_corpus.append(' '.join([stemmer.stem(x) for x in doc.split()]))\n",
    "        corpus = stemmed_corpus\n",
    "    else:\n",
    "        corpus = df['document']\n",
    "\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=1.0, min_df=2, strip_accents='ascii', stop_words='english',\n",
    "                                 token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
    "    dtm = vectorizer.fit_transform(corpus)\n",
    "    print('DTM shape:', dtm.shape)\n",
    "    wordfreq = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "    print('Top 20 words')\n",
    "    print(sorted(wordfreq.items(), key=lambda x: x[1])[:20])\n",
    "    reference_vocab = vectorizer.get_feature_names()\n",
    "    print(len(reference_vocab), 'words')\n",
    "\n",
    "    X, y = dtm, df['cell_id']\n",
    "    clf = LinearSVC(penalty='l2', multi_class='ovr', random_state=42)\n",
    "    X_new = SelectFromModel(estimator=clf, threshold='median').fit_transform(X, y)\n",
    "    print(X.shape, X_new.shape)\n",
    "\n",
    "    clf = LinearSVC(penalty=\"l2\", dual=False, tol=1e-3, multi_class='ovr', C=1)\n",
    "    print('=' * 80)\n",
    "\n",
    "    scoring = ['precision_macro', 'precision_micro', 'recall_macro', 'recall_micro']\n",
    "    results = cross_validate(estimator=clf, X=X_new, y=y, cv=10, n_jobs=-1, verbose=0,\n",
    "                             return_estimator=True, return_train_score=True, scoring=scoring, groups=df['cell_id'])\n",
    "    print(results)\n",
    "    return {'clf': clf, 'results': results, 'pm_count': pm_count}\n",
    "\n",
    "\n",
    "for min_ref in [10, 20, 30, 40, 50, 60, 70, 80, 90, 100]:\n",
    "    print('_'*80)\n",
    "    print(min_ref, 'minimum references')\n",
    "    results_min_ref[min_ref] = min_ref_pipeline(reference_df, min_references=min_ref, stemmed=False)\n",
    "\n",
    "\n",
    "x = sorted(list(results_min_ref.keys()))\n",
    "y = [len(results_min_ref[min_ref]['pm_count']) for min_ref in results_min_ref.keys()]\n",
    "min_ref_pd = pd.DataFrame(data={'Minimum number of documents': x, 'Number of cell lines': y})\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.ylabel('Number of cell lines')\n",
    "plt.xlabel('Minimum number of documents')\n",
    "plt.scatter(x, y, linestyle='None', marker='x')\n",
    "plt.savefig('data/img/min_ref.png')\n",
    "plt.show()\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.ylabel('Averaged Micro Precision')\n",
    "plt.xlabel('Minimum number of documents')\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "x = sorted(list(results_min_ref.keys()))\n",
    "y = [np.average(results_min_ref[min_ref]['results']['train_precision_micro']) for min_ref in x]\n",
    "e = [np.std(results_min_ref[min_ref]['results']['train_precision_micro']) for min_ref in x]\n",
    "plt.errorbar(x, y, e, linestyle='None', marker=markers[i], label='Train dataset', capsize=3)\n",
    "\n",
    "y = [np.average(results_min_ref[min_ref]['results']['test_precision_micro']) for min_ref in x]\n",
    "e = [np.std(results_min_ref[min_ref]['results']['test_precision_micro']) for min_ref in x]\n",
    "plt.errorbar(x, y, e, linestyle='None', marker=markers[i], label='Test dataset', capsize=3)\n",
    "\n",
    "plt.ylim(0, 1)\n",
    "plt.legend(loc='upper left')\n",
    "plt.savefig('data/img/precision_min_ref.png')\n",
    "plt.show()\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we define a pipeline for calculating Micro-Averaged precision varying the variance threshold for feature selection.\n",
    "We plot micro-averaged precision for Non-Stemmed corpus.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "def min_threshold_pipeline(df, min_references, stemmed=False):\n",
    "    pm_count = df.groupby(by=['cell_id'])['cell_id'].count()\n",
    "    df = df[df.cell_id.isin(pm_count[pm_count >= min_references].index)]\n",
    "    df = df[df.cell_id.isin(pm_count[pm_count < 500].index)]\n",
    "    print(df.shape[0], 'abstracts in corpus')\n",
    "    pm_count = df.groupby(by=['cell_id'])['cell_id'].count()\n",
    "    print(pm_count.shape[0], 'cell lines')\n",
    "    print('Every cell line has at least', min(pm_count), 'documents')\n",
    "\n",
    "    if stemmed:\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmed_corpus = list()\n",
    "        for doc in df['document']:\n",
    "            doc = doc.replace('.', '').replace(',', '').replace(';', '').replace(':', '')\n",
    "            stemmed_corpus.append(' '.join([stemmer.stem(x) for x in doc.split()]))\n",
    "        corpus = stemmed_corpus\n",
    "    else:\n",
    "        corpus = df['document']\n",
    "\n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=1.0, min_df=2, strip_accents='ascii', stop_words='english',\n",
    "                                 token_pattern=u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b')\n",
    "    dtm = vectorizer.fit_transform(corpus)\n",
    "    print('DTM shape:', dtm.shape)\n",
    "    wordfreq = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n",
    "    print('Top 20 words')\n",
    "    print(sorted(wordfreq.items(), key=lambda x: x[1])[:20])\n",
    "    reference_vocab = vectorizer.get_feature_names()\n",
    "    print(len(reference_vocab), 'words')\n",
    "\n",
    "    X, y = dtm, df['cell_id']\n",
    "    clf = LinearSVC(penalty='l2', multi_class='ovr', random_state=42)\n",
    "    thresholds = ['0.5*median', 'median', '1.5*median', '2*median']\n",
    "    results = dict()\n",
    "    for threshold in thresholds:\n",
    "        print(threshold)\n",
    "        X_new = SelectFromModel(estimator=clf, threshold=threshold).fit_transform(X, y)\n",
    "        print(X.shape, X_new.shape)\n",
    "\n",
    "        clf = LinearSVC(penalty=\"l2\", dual=False, tol=1e-3, multi_class='ovr', C=1)\n",
    "        print('=' * 80)\n",
    "\n",
    "        scoring = ['precision_macro', 'precision_micro', 'recall_macro', 'recall_micro']\n",
    "        results[threshold] = cross_validate(estimator=clf, X=X_new, y=y, cv=10, n_jobs=-1, verbose=0,\n",
    "                                 return_estimator=True, return_train_score=True, scoring=scoring, groups=df['cell_id'])\n",
    "    return {'results': results}\n",
    "\n",
    "\n",
    "results_min_ref_ = dict()\n",
    "for min_ref in [60, 70, 80, 90, 100]:\n",
    "    print('_'*80)\n",
    "    print(min_ref, 'minimum references')\n",
    "    results_min_ref_[min_ref] = min_threshold_pipeline(reference_df, min_references=min_ref, stemmed=False)\n",
    "\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.ylabel('Averaged Micro Precision')\n",
    "plt.xlabel('Minimum number of documents')\n",
    "markers = ['^', '.', 'v', 's']\n",
    "thresholds = ['0.5*median', 'median', '1.5*median', '2*median']\n",
    "labels = [' (threshold = 0.5*median)', ' (threshold = median)', ' (threshold = 1.5*median)', ' (threshold = 2*median)']\n",
    "\n",
    "x = sorted(list(results_min_ref_.keys()))\n",
    "for i in range(4):\n",
    "    y = [np.average(results_min_ref_[min_ref]['results'][thresholds[i]]['train_precision_micro']) for min_ref in x]\n",
    "    e = [np.std(results_min_ref_[min_ref]['results'][thresholds[i]]['train_precision_micro']) for min_ref in x]\n",
    "    plt.errorbar(x, y, e, linestyle='None', marker='^', label='Train dataset'+labels[i], capsize=3)\n",
    "\n",
    "    y = [np.average(results_min_ref_[min_ref]['results'][thresholds[i]]['test_precision_micro']) for min_ref in x]\n",
    "    e = [np.std(results_min_ref_[min_ref]['results'][thresholds[i]]['test_precision_micro']) for min_ref in x]\n",
    "    plt.errorbar(x, y, e, linestyle='None', marker='o', label='Test dataset'+labels[i], capsize=3)\n",
    "\n",
    "plt.ylim(0.2, 1)\n",
    "plt.legend(loc='lower left')\n",
    "\n",
    "plt.savefig('data/img/precision_min_ref_threshold.png')\n",
    "plt.close()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}